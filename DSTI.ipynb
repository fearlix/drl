{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "q9rC7Sen6zOW",
        "ymJ19CPxsjoa",
        "Nm_5Mj3AxacV",
        "VVXF-Q7ZygIp",
        "JWGWi6uY_uWi",
        "VWV8AWz6ArHo",
        "oHITDL9fz3ps",
        "vpRyZup6LWiu"
      ],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNWMob8UiaZasTDCqhUe23d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fearlix/drl/blob/main/DSTI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Model for Image Classification for planes**"
      ],
      "metadata": {
        "id": "ZvpXUret6sMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Imports & Setup**"
      ],
      "metadata": {
        "id": "q9rC7Sen6zOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets torchvision torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N5IqAsS63eA",
        "outputId": "44378f20-87f3-49b8-aeb1-10bdc697e4fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
        "from huggingface_hub import HfApi, login, whoami,create_repo, upload_file, list_repo_files, hf_hub_download\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score\n",
        ")"
      ],
      "metadata": {
        "id": "M14wgIZB66ki"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data Loading & Labeling**"
      ],
      "metadata": {
        "id": "ymJ19CPxsjoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loads two datasets from Hugging Face: one for planes and one for cars. It prints both datasets to check if they were loaded correctly and then returns them for further use."
      ],
      "metadata": {
        "id": "zOMPD-ka80PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets():\n",
        "    \"\"\"Loads the datasets from Hugging Face.\"\"\"\n",
        "    planes_dataset = load_dataset(\"fearlixg/planes_splitted\")\n",
        "    cars_dataset = load_dataset(\"fearlixg/cars_splitted\")\n",
        "    return planes_dataset, cars_dataset"
      ],
      "metadata": {
        "id": "TpDoBF7245ZP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function adds labels to the datasets: **1 for planes** and **0 for cars**. It does this by applying a small helper function that adds a label field to each example. Then, it updates both the training and test sets of each dataset with the correct labels. Finally, it returns the updated datasets."
      ],
      "metadata": {
        "id": "ppXSH9Wo83-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_datasets(planes_dataset, cars_dataset):\n",
        "    \"\"\"Assigns labels: Planes (1), Cars (0).\"\"\"\n",
        "    def add_label(example, label):\n",
        "        example[\"label\"] = label\n",
        "        return example\n",
        "\n",
        "    for dataset in [planes_dataset, cars_dataset]:\n",
        "        label = 1 if dataset == planes_dataset else 0\n",
        "        dataset[\"train\"] = dataset[\"train\"].map(lambda x: add_label(x, label))\n",
        "        dataset[\"test\"] = dataset[\"test\"].map(lambda x: add_label(x, label))\n",
        "\n",
        "    return planes_dataset, cars_dataset"
      ],
      "metadata": {
        "id": "FoNMAxwk85dV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Merge & Balance the Dataset**"
      ],
      "metadata": {
        "id": "Nm_5Mj3AxacV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function **merges** the plane and car datasets into a single dataset and then **balances** it. If one class has fewer examples, it adds more samples by randomly duplicating them until both classes have the same amount. This helps ensure the model learns equally from both planes and cars. Finally, it returns the balanced dataset."
      ],
      "metadata": {
        "id": "1RnMXCLMiTIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_and_balance_datasets(planes_dataset, cars_dataset):\n",
        "    \"\"\"Merges and balances the datasets by oversampling.\"\"\"\n",
        "    train_dataset = concatenate_datasets([planes_dataset[\"train\"], cars_dataset[\"train\"]])\n",
        "    test_dataset = concatenate_datasets([planes_dataset[\"test\"], cars_dataset[\"test\"]])\n",
        "\n",
        "    final_dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "    # Balance dataset\n",
        "    plane_count = sum(1 for label in final_dataset[\"train\"][\"label\"] if label == 1)\n",
        "    car_count = sum(1 for label in final_dataset[\"train\"][\"label\"] if label == 0)\n",
        "    diff = plane_count - car_count\n",
        "\n",
        "    if diff > 0:\n",
        "        car_indices = [i for i, label in enumerate(final_dataset[\"train\"][\"label\"]) if label == 0]\n",
        "        additional_indices = random.choices(car_indices, k=diff)\n",
        "        additional_car_dataset = final_dataset[\"train\"].select(additional_indices)\n",
        "        final_dataset[\"train\"] = concatenate_datasets([final_dataset[\"train\"], additional_car_dataset])\n",
        "\n",
        "    return final_dataset"
      ],
      "metadata": {
        "id": "jDJJbOwY9thg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.  Data Augmentation & Transformations**"
      ],
      "metadata": {
        "id": "VVXF-Q7ZygIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function creates image transformations for training and testing.  \n",
        "\n",
        "- **Training:** It resizes images, randomly flips, rotates, adjusts colors, blurs, and normalizes them. This helps the model learn better by seeing different variations.  \n",
        "- **Testing:** It only resizes and normalizes images to keep them consistent.  \n",
        "\n",
        "Both transformations return images in the correct format for the model."
      ],
      "metadata": {
        "id": "QLKS9pDWicQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms():\n",
        "    \"\"\"Returns train and test transformations.\"\"\"\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomRotation(degrees=30),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.3), ratio=(0.2, 3.0)),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "        transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return transform_train, transform_test"
      ],
      "metadata": {
        "id": "g83NPA6w-2i2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.  Create DataLoaders**"
      ],
      "metadata": {
        "id": "JWGWi6uY_uWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class **creates a custom dataset** for Hugging Face images so they can be used in PyTorch."
      ],
      "metadata": {
        "id": "2WWkCpyBijKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuggingFaceImageDataset(Dataset):\n",
        "    \"\"\"Custom dataset class for Hugging Face datasets.\"\"\"\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item[\"image\"]\n",
        "\n",
        "        if not hasattr(image, \"convert\"):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, item[\"label\"]"
      ],
      "metadata": {
        "id": "Ie0WIOWi_s0H"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets(final_dataset, transform_train, transform_test):\n",
        "    \"\"\"Creates DataLoaders for training and testing.\"\"\"\n",
        "    train_data = HuggingFaceImageDataset(final_dataset[\"train\"], transform=transform_train)\n",
        "    test_data = HuggingFaceImageDataset(final_dataset[\"test\"], transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "N5liSZD3YqO1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.  Define additional functions**"
      ],
      "metadata": {
        "id": "VWV8AWz6ArHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "odNAMw1oisDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_device():\n",
        "    \"\"\"Returns the available device (GPU or CPU).\"\"\"\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NSS4d31FVqcZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(device):\n",
        "    \"\"\"Initializes and modifies the ResNet50 model.\"\"\"\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for layer in [model.layer3, model.layer4]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(model.fc.in_features, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(512, 2)\n",
        "    )\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "def setup_training_components(model):\n",
        "    \"\"\"Returns loss function, optimizer, and scheduler.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "    return criterion, optimizer, scheduler"
      ],
      "metadata": {
        "id": "Ar74dUQeVvWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, device, alpha=0.05):\n",
        "    \"\"\"Applies MixUp augmentation to improve generalization.\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0)).to(device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Computes loss for MixUp augmented images.\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ],
      "metadata": {
        "id": "U430BKtGAzcK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.  Define Model Training**"
      ],
      "metadata": {
        "id": "oHITDL9fz3ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function **trains a model** while using **MixUp augmentation** and **early stopping** to improve learning.  \n",
        "\n",
        "1. **Setup**: It starts with the best validation loss set to infinity and an early stopping counter.  \n",
        "2. **Training Loop**:  \n",
        "   - Runs for a set number of **epochs**.  \n",
        "   - Goes through the **training data**, mixing images and labels using **MixUp**.  \n",
        "   - The model makes predictions and updates its weights to improve.  \n",
        "   - Tracks training accuracy after each epoch.  \n",
        "3. **Learning Rate Adjustment**: The **scheduler** updates the learning rate to keep training stable.  \n",
        "4. **Early Stopping**: If the model doesn’t improve for too long, it **stops early** to prevent overfitting.  \n",
        "5. **Returns the trained model** after finishing training.  \n"
      ],
      "metadata": {
        "id": "kLICRg_Ci_is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, epochs=1, patience=5):\n",
        "    \"\"\"Trains the model with early stopping and MixUp, and returns the trained model.\"\"\"\n",
        "    best_val_loss = float(\"inf\")\n",
        "    stopping_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, device)\n",
        "            outputs = model(mixed_images)\n",
        "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        if stopping_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "55cFZUklA7BW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.  Define Validation**"
      ],
      "metadata": {
        "id": "vpRyZup6LWiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function evaluates the model to check how well it performs on test data.\n",
        "\n",
        "Setup: The model switches to evaluation mode and tracking variables are initialized.\n",
        "Testing Loop:\n",
        "Goes through all test images without updating the model.\n",
        "Makes predictions and calculates probabilities.\n",
        "Keeps track of correct predictions for accuracy.\n",
        "Metrics Calculation:\n",
        "Computes accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "Uses a confusion matrix to show how well the model classifies each category.\n",
        "Results Display:\n",
        "Prints key performance metrics.\n",
        "Shows a classification report and a confusion matrix plot.\n",
        "Returns accuracy so it can be used elsewhere."
      ],
      "metadata": {
        "id": "-MVHkOLljLhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluates the model and prints performance metrics.\"\"\"\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_labels, all_predictions, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Automatically detect number of classes\n",
        "    unique_classes = sorted(set(all_labels))\n",
        "    num_classes = len(unique_classes)\n",
        "\n",
        "    # Dynamically create class names\n",
        "    target_names = [f\"Class {i}\" for i in unique_classes]\n",
        "\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\")\n",
        "\n",
        "    print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "vHwX7e-AIw6J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Uploading best model**"
      ],
      "metadata": {
        "id": "bVXEQsQ7Q1z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "js4YHrOEjcpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the Hugging Face token from Colab secrets\n",
        "def get_huggingface_token():\n",
        "    \"\"\"Fetches the stored Hugging Face token from Google Colab secrets, logs in, and returns the token.\"\"\"\n",
        "    from google.colab import userdata\n",
        "    from huggingface_hub import login  # Ensure you have huggingface_hub installed\n",
        "\n",
        "    hf_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"Successfully logged in to Hugging Face!\")\n",
        "        return hf_token\n",
        "    else:\n",
        "        print(\"Hugging Face token not found. Please set it manually in Colab secrets.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_repo_id_and_model(hf_token, model_name):\n",
        "    \"\"\"Generates the repository ID and model filename for Hugging Face models.\"\"\"\n",
        "    try:\n",
        "        user_info = whoami(token=hf_token)\n",
        "        username = user_info.get(\"name\") or user_info.get(\"login\", \"Unknown User\")\n",
        "\n",
        "        if username == \"Unknown User\":\n",
        "            print(\"Error: Could not retrieve username. Please check your token.\")\n",
        "            return None, None\n",
        "\n",
        "        repo_id = f\"{username}/{model_name}\"\n",
        "        model_filename = f\"best_{model_name}\"\n",
        "\n",
        "        print(\"Repo ID:\", repo_id)\n",
        "        print(\"Model Filename:\", model_filename)\n",
        "\n",
        "        return repo_id, model_filename\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching repo ID: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def upload_new_model_with_timestamp(model, repo_id, model_name, hf_token=None):\n",
        "    \"\"\"Uploads the model with a timestamp to Hugging Face.\"\"\"\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "    else:\n",
        "        print(\"Hugging Face token is missing.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        list_repo_files(repo_id, token=hf_token)\n",
        "    except Exception:\n",
        "        create_repo(repo_id, exist_ok=True, token=hf_token)\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_filename = f\"{model_name}_{timestamp}.pth\"\n",
        "\n",
        "    torch.save(model.state_dict(), model_filename)\n",
        "    upload_file(path_or_fileobj=model_filename, path_in_repo=model_filename, repo_id=repo_id, token=hf_token)\n",
        "    print(f\"Model uploaded as {repo_id}/{model_filename}\")\n"
      ],
      "metadata": {
        "id": "HwilWrTWQ72m"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.  Main Function**"
      ],
      "metadata": {
        "id": "auwk7XesBWiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function runs the full training and evaluation process step by step."
      ],
      "metadata": {
        "id": "ZHvME-BHjiEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Runs the full training and evaluation pipeline.\"\"\"\n",
        "    print(\"🔹 Getting Hugging Face authentication...\")\n",
        "    hf_token = get_huggingface_token()\n",
        "\n",
        "    model_name = \"cars_vs_planes_model\"\n",
        "\n",
        "    repo_id, model_filename = get_repo_id_and_model(hf_token, model_name)\n",
        "\n",
        "    device = setup_device()\n",
        "\n",
        "    print(\"🔹 Loading datasets...\")\n",
        "    planes_dataset, cars_dataset = load_datasets()\n",
        "\n",
        "    print(\"🔹 Labeling datasets...\")\n",
        "    planes_dataset, cars_dataset = label_datasets(planes_dataset, cars_dataset)\n",
        "\n",
        "    print(\"🔹 Merging and balancing datasets...\")\n",
        "    final_dataset = merge_and_balance_datasets(planes_dataset, cars_dataset)\n",
        "\n",
        "    print(\"🔹 Applying data transformations...\")\n",
        "    transform_train, transform_test = get_transforms()\n",
        "\n",
        "    print(\"🔹 Creating DataLoaders...\")\n",
        "    train_loader, test_loader = prepare_datasets(final_dataset, transform_train, transform_test)\n",
        "\n",
        "    print(\"🔹 Initializing model...\")\n",
        "    model = setup_model(device)\n",
        "    criterion, optimizer, scheduler = setup_training_components(model)\n",
        "\n",
        "    print(\"🔹 Training the model...\")\n",
        "    trained_model = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, epochs=1)\n",
        "\n",
        "    print(\"🔹 validating the best model...\")\n",
        "    validate_model(trained_model, test_loader, device)\n",
        "\n",
        "    print(\"🔹 Uploading the trained model...\")\n",
        "    upload_new_model_with_timestamp(trained_model, repo_id, model_filename, hf_token)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvlxJl7gBO4N",
        "outputId": "7788532b-5454-4db1-b9a5-348de2dde6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Getting Hugging Face authentication...\n",
            "Successfully logged in to Hugging Face!\n",
            "Repo ID: fearlixg/cars_vs_planes_model\n",
            "Model Filename: best_cars_vs_planes_model\n",
            "🔹 Loading datasets...\n",
            "🔹 Labeling datasets...\n",
            "🔹 Merging and balancing datasets...\n",
            "🔹 Applying data transformations...\n",
            "🔹 Creating DataLoaders...\n",
            "🔹 Initializing model...\n",
            "🔹 Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  38%|███▊      | 189/500 [02:30<04:04,  1.27it/s]"
          ]
        }
      ]
    }
  ]
}